{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manjotmb20/tweet-analysis/blob/master/FinalModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e65ZEqdNdff-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ec409112-03fb-441d-9438-37b0c7e07b77"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuK42r2jd1fl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "aced6299-6f65-48ac-f335-210b544029e8"
      },
      "source": [
        "data=pd.read_csv(\"tweets.csv\")\n",
        "data.dropna(inplace=True,axis=0)\n",
        "data.drop([\"Unnamed: 0\"],axis=1,inplace=True)\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/f8/810ec35c31cca89bc4f1a02c14b042b9ec6c19dd21f7ef1876874ef069a6/tweet-preprocessor-0.5.0.tar.gz\n",
            "Building wheels for collected packages: tweet-preprocessor\n",
            "  Building wheel for tweet-preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tweet-preprocessor: filename=tweet_preprocessor-0.5.0-cp36-none-any.whl size=7947 sha256=97f9c2779cbdd756c0b59ec03a717d6d6093129c545771c9333351401c01221f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/27/cc/49938e98a2470802ebdefae9d2b3f524768e970c1ebbe2dc4a\n",
            "Successfully built tweet-preprocessor\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9lYFM4BeHE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, ax = plt.subplots(figsize=(10, 8))\n",
        "corr = data.corr()\n",
        "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "            square=True, ax=ax,annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR7C2ekdeTvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list2=[]\n",
        "for i in data.lang.unique():\n",
        "  if len(data[data.lang==i])>100 and i!=\"und\":\n",
        "    list2.append(i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYSaCdlbi6Zv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "25a94e6a-ffe3-4a61-8a33-e2a6223b6d44"
      },
      "source": [
        "!pip install emoji --upgrade\n",
        "import emoji"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=ec6732b31c3bed0399ff6787a1fa57f5e3e07c69599fe781d0aae0ad72aabdaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnWPBg35muBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_score(y_val,y_pred):\n",
        "  print(\"accuracy score :\", accuracy_score(y_val,y_pred))\n",
        "  print(\"confusion matrix:\",confusion_matrix(y_val,y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NlwZ-3NlVbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score\n",
        "import xgboost as xgb\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5es5UAJpIB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    #converting all to lower case\n",
        "    tweet=tweet.lower()\n",
        "    #converting all tweet urls to string \"URL\"\n",
        "    tweet=re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
        "    #converting all @username to \"AT_USER\"\n",
        "    tweet=re.sub('@[^\\s]+','AT_USER',tweet)\n",
        "    #removing numbers from tweets\n",
        "    tweet= re.sub(r'\\d+','', tweet)\n",
        "    #converting all multiple white spaces to a single space\n",
        "    tweet=re.sub('[\\s]+',' ',tweet)\n",
        "    #converting hastags to name after hastags\n",
        "    tweet=re.sub(r'#([^\\s]+)',r'Hashtag',tweet)\n",
        "    #converting emojis to words\n",
        "    tweet=emoji.demojize(tweet)\n",
        "    tweet = re.sub(r'[?|$|.|!|ÿ|œ¹|ð]','',tweet)\n",
        "    #Removing special characters such as punctuation from tweets\n",
        "    tweet=tweet.replace(r'[^a-zA-Z_]','')\n",
        "    #Stemming words in text\n",
        "    word=' '.join(([word for word in tweet.split()]))\n",
        "    return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLKeNzyHpWIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=1000,ngram_range=(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIjabKhDz39N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt_clf_en=DecisionTreeClassifier()\n",
        "dt_clf_ar=DecisionTreeClassifier()\n",
        "dt_clf_es=DecisionTreeClassifier()\n",
        "dt_clf_pt=DecisionTreeClassifier()\n",
        "dt_clf_in=DecisionTreeClassifier()\n",
        "dt_clf_fr=DecisionTreeClassifier()\n",
        "dtclf_dict=({'ar':dt_clf_ar,'en':dt_clf_en,'es':dt_clf_es,'fr':dt_clf_fr,'pt':dt_clf_pt,'in':dt_clf_in})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7ivJfws2Y0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_clf_en=LogisticRegression()\n",
        "lr_clf_ar=LogisticRegression()\n",
        "lr_clf_es=LogisticRegression()\n",
        "lr_clf_pt=LogisticRegression()\n",
        "lr_clf_in=LogisticRegression()\n",
        "lr_clf_fr=LogisticRegression()\n",
        "lrclf_dict=({'ar':lr_clf_ar,'en':lr_clf_en,'es':lr_clf_es,'fr':lr_clf_fr,'pt':lr_clf_pt,'in':lr_clf_in})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvqG1a1i7djA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_en = TfidfVectorizer(max_features=1000,ngram_range=(1,1))\n",
        "tfidf_ar = TfidfVectorizer(max_features=1000,ngram_range=(1,1))\n",
        "tfidf_es = TfidfVectorizer(max_features=1000,ngram_range=(1,1))\n",
        "tfidf_pt = TfidfVectorizer(max_features=1000,ngram_range=(1,1))\n",
        "tfidf_in = TfidfVectorizer(max_features=1000,ngram_range=(1,1))\n",
        "tfidf_fr = TfidfVectorizer(max_features=1000,ngram_range=(1,1))\n",
        "tfidf_dict=({'ar':tfidf_ar,'en':tfidf_en,'es':tfidf_es,'fr':tfidf_fr,'pt':tfidf_pt,'in':tfidf_in})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8eoqcKrea83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0158827-b34f-4e75-8792-4db0c3c14eb6"
      },
      "source": [
        "lang_dict=({'en':'english','es':'spanish','fr':'french','pt':'portuguese','in':'indonesian','ar':'arabic'})\n",
        "for lang in list2:\n",
        "  df_eng=pd.DataFrame()\n",
        "  df_eng=data[data.lang==lang]\n",
        "  from nltk.corpus import stopwords\n",
        "  stop = stopwords.words(lang_dict[lang])\n",
        "  from nltk.stem import PorterStemmer\n",
        "  final_list=[]\n",
        "  labels=df_eng.label\n",
        "  for row in df_eng[\"tweet_text\"]:\n",
        "    final_list.append((preprocess_tweet(row)))\n",
        "  df_new=pd.DataFrame({'clean_text':final_list,'label':labels})\n",
        "  x_train,x_val,y_train,y_val=train_test_split(df_new['clean_text'],df_new['label'],test_size=0.2,random_state=9)\n",
        "  xtrain_tfidf=tfidf_dict[lang].fit_transform(x_train)\n",
        "  xval_tfidf=tfidf_dict[lang].transform(x_val)\n",
        "  print(\"Shape of train data for lang {} is {}\".format(lang_dict[lang],xtrain_tfidf.shape))\n",
        "  print(\"Shape of test data for lang {} is {}\".format(lang_dict[lang],xval_tfidf.shape))\n",
        "  print(\"Logistic Regression Model\")\n",
        "  lrclf_dict[lang].fit(xtrain_tfidf,y_train)\n",
        "  y_pred=lrclf_dict[lang].predict(xval_tfidf)\n",
        "  print_score(y_val,y_pred)\n",
        "  print(\"Decision Tree Classifier\")\n",
        "  dtclf_dict[lang].fit(xtrain_tfidf,y_train)\n",
        "  y_pred=dtclf_dict[lang].predict(xval_tfidf)\n",
        "  print_score(y_val,y_pred)\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train data for lang english is (5881, 1000)\n",
            "Shape of test data for lang english is (1471, 1000)\n",
            "Logistic Regression Model\n",
            "accuracy score: 0.7824609109449354\n",
            "confusion matrix: [[435 180]\n",
            " [140 716]]\n",
            "Decision Tree Classifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy score: 0.7273963290278722\n",
            "confusion matrix: [[435 180]\n",
            " [221 635]]\n",
            "Shape of train data for lang french is (97, 656)\n",
            "Shape of test data for lang french is (25, 656)\n",
            "Logistic Regression Model\n",
            "accuracy score: 0.72\n",
            "confusion matrix: [[17  0]\n",
            " [ 7  1]]\n",
            "Decision Tree Classifier\n",
            "accuracy score: 0.84\n",
            "confusion matrix: [[16  1]\n",
            " [ 3  5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of train data for lang spanish is (489, 1000)\n",
            "Shape of test data for lang spanish is (123, 1000)\n",
            "Logistic Regression Model\n",
            "accuracy score: 0.8048780487804879\n",
            "confusion matrix: [[99  0]\n",
            " [24  0]]\n",
            "Decision Tree Classifier\n",
            "accuracy score: 0.7967479674796748\n",
            "confusion matrix: [[87 12]\n",
            " [13 11]]\n",
            "Shape of train data for lang portuguese is (269, 1000)\n",
            "Shape of test data for lang portuguese is (68, 1000)\n",
            "Logistic Regression Model\n",
            "accuracy score: 0.7794117647058824\n",
            "confusion matrix: [[52  2]\n",
            " [13  1]]\n",
            "Decision Tree Classifier\n",
            "accuracy score: 0.7647058823529411\n",
            "confusion matrix: [[46  8]\n",
            " [ 8  6]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of train data for lang indonesian is (110, 960)\n",
            "Shape of test data for lang indonesian is (28, 960)\n",
            "Logistic Regression Model\n",
            "accuracy score: 0.8571428571428571\n",
            "confusion matrix: [[24  0]\n",
            " [ 4  0]]\n",
            "Decision Tree Classifier\n",
            "accuracy score: 0.9285714285714286\n",
            "confusion matrix: [[24  0]\n",
            " [ 2  2]]\n",
            "Shape of train data for lang arabic is (143, 1000)\n",
            "Shape of test data for lang arabic is (36, 1000)\n",
            "Logistic Regression Model\n",
            "accuracy score: 0.7777777777777778\n",
            "confusion matrix: [[28  0]\n",
            " [ 8  0]]\n",
            "Decision Tree Classifier\n",
            "accuracy score: 0.7777777777777778\n",
            "confusion matrix: [[25  3]\n",
            " [ 5  3]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBoSrQ8LiQoc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "859b390e-cd31-453e-9b97-131689e46a0e"
      },
      "source": [
        "test_list=[]\n",
        "test=\"Picking up Togo order from wings n ale they have cleaned this AT_USER AT_USER #place up, looks good , https://ehfhefbest.com  https://ehfhefbest.com wings n lex \"\n",
        "test_list.append(preprocess_tweet(test))\n",
        "tf_idf=tfidf_dict['en'].transform(test_list)\n",
        "lrclf_dict['en'].predict(tf_idf)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Genuine'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFssNW8EqvDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}